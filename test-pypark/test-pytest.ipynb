{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 4.8.3\n",
      "  latest version: 4.8.4\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base conda\n",
      "\n",
      "\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /opt/conda\n",
      "\n",
      "  added / updated specs:\n",
      "    - ipytest\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    ipytest-0.9.1              |             py_0          14 KB  conda-forge\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:          14 KB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  ipytest            conda-forge/noarch::ipytest-0.9.1-py_0\n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "ipytest-0.9.1        | 14 KB     | ##################################### | 100% \n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n"
     ]
    }
   ],
   "source": [
    "!conda install -y ipytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipytest\n",
    "ipytest.autoconfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, absolute_import\n",
    "\n",
    "#import findspark\n",
    "#findspark.init()\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import HiveContext\n",
    "import pyspark.sql.functions as sf\n",
    "\n",
    "\n",
    "def define_df(hive_context):\n",
    "    \"Dummy function\"\n",
    "    data = [('foo', 1),\n",
    "            ('bar', 2)]\n",
    "    schema = get_schema()\n",
    "    df = hive_context.createDataFrame(data, schema=schema)\n",
    "    return df\n",
    "\n",
    "\n",
    "def filter_str(df, column, string):\n",
    "    return df.filter(~(sf.regexp_extract(column, '({})'.format(string), 1) == string))\n",
    "\n",
    "\n",
    "def get_schema():\n",
    "    return ['name', 'age']\n",
    "\n",
    "def main(sc, hc):\n",
    "    df = define_df(hc).coalesce(1)\n",
    "    filtered = filter_str(df, 'name', 'bar')\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    sc = SparkContext.getOrCreate()\n",
    "    hc = HiveContext(sc)\n",
    "    main(sc, hc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "import pytest\n",
    "#import findspark\n",
    "#findspark.init()\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import HiveContext\n",
    "\n",
    "\n",
    "APP_NAME = 'pytest-pyspark-tests'\n",
    "\n",
    "\n",
    "@pytest.fixture(scope=\"session\", autouse=True)\n",
    "def spark_context(request):\n",
    "    \"\"\"Fixture to create the SparkContext.\n",
    "    The tests run on real data, so it should run on the cluster\n",
    "    where the actual data is present.\n",
    "    Args:\n",
    "        request: pytest.FixtureRequest object\n",
    "    Returns:\n",
    "        HiveContext for tests\n",
    "    \"\"\"\n",
    "    conf = SparkConf().setAppName(APP_NAME)\n",
    "    sc = SparkContext.getOrCreate(conf=conf)\n",
    "    sc.setLogLevel(\"ERROR\")\n",
    "    # request.addfinalizer(lambda: sc.stop())\n",
    "    yield sc\n",
    "    sc.stop()\n",
    "\n",
    "\n",
    "@pytest.fixture(scope=\"session\", autouse=True)\n",
    "def hive_context(spark_context, request):\n",
    "    \"\"\"\n",
    "    Fixture to create the HiveContext.\n",
    "    Args:\n",
    "        spark_context: spark_context fixture\n",
    "    Returns:\n",
    "    HiveContext\n",
    "    \"\"\"\n",
    "    try:\n",
    "        shutil.rmtree('metastore_db')\n",
    "    except:\n",
    "        pass\n",
    "    # request.addfinalizer(lambda: spark_context.stop())\n",
    "    yield HiveContext(spark_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...                                                                                                                                         [100%]\n",
      "================================================================ warnings summary =================================================================\n",
      "tmp4ce73nbs.py::test_define_df\n",
      "  /usr/local/spark/python/pyspark/sql/context.py:496: DeprecationWarning: HiveContext is deprecated in Spark 2.0.0. Please use SparkSession.builder.enableHiveSupport().getOrCreate() instead.\n",
      "    warnings.warn(\n",
      "\n",
      "tmp4ce73nbs.py::test_define_df\n",
      "tmp4ce73nbs.py::test_define_df\n",
      "  /usr/local/spark/python/pyspark/sql/context.py:75: DeprecationWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "    warnings.warn(\n",
      "\n",
      "-- Docs: https://docs.pytest.org/en/stable/warnings.html\n",
      "3 passed, 3 warnings in 3.61s\n"
     ]
    }
   ],
   "source": [
    "%%run_pytest\n",
    "\n",
    "from __future__ import print_function, absolute_import\n",
    "import pytest\n",
    "\n",
    "#import findspark\n",
    "#findspark.init()\n",
    "\n",
    "import pyspark.sql.functions as sf  # more import if needed here\n",
    "\n",
    "\n",
    "\n",
    "def test_define_df(spark_context, hive_context):\n",
    "    df = define_df(hive_context).coalesce(1)\n",
    "    row, = df.select(sf.max('age')).collect()\n",
    "    assert row['max(age)'] == 2\n",
    "\n",
    "\n",
    "def test_filter_str(hive_context):\n",
    "    df = define_df(hive_context).coalesce(1)\n",
    "    filtered = filter_str(df, 'name', 'bar')\n",
    "    row, = filtered.collect()\n",
    "    assert row['name'] == 'foo'\n",
    "\n",
    "\n",
    "def test_get_schema():\n",
    "    assert ['name', 'age'] == get_schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "def suppress_py4j_logging():\n",
    "    logger = logging.getLogger(‘py4j’)\n",
    "    logger.setLevel(logging.WARN)\n",
    "    \n",
    "def create_testing_pyspark_session():\n",
    "    return (SparkSession.builder\n",
    "    .master(‘local[2]’)\n",
    "    .appName(‘my-local-testing-pyspark-context’)\n",
    "    .enableHiveSupport()\n",
    "    .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unittest\n",
    "import logging\n",
    "from pyspark.sql import SparkSessionclass PySparkTest(unittest.TestCase):\n",
    " \n",
    "@classmethod\n",
    "def suppress_py4j_logging(cls):\n",
    "    logger = logging.getLogger(‘py4j’)\n",
    "    logger.setLevel(logging.WARN)\n",
    "    \n",
    "@classmethod\n",
    "def create_testing_pyspark_session(cls):\n",
    "    return (SparkSession.builder\n",
    "    .master(‘local[2]’)\n",
    "    .appName(‘my-local-testing-pyspark-context’)\n",
    "    .enableHiveSupport()\n",
    "    .getOrCreate())\n",
    " \n",
    "@classmethod\n",
    "def setUpClass(cls):\n",
    "    cls.suppress_py4j_logging()\n",
    "    cls.spark = cls.create_testing_pyspark_session()\n",
    "\n",
    "@classmethod\n",
    "def tearDownClass(cls):\n",
    "    cls.spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_spark_context_fixture(spark_context):\n",
    "    test_rdd = spark_context.parallelize([1, 2, 3, 4])\n",
    "\n",
    "    assert test_rdd.count() == 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E\n",
      "======================================================================\n",
      "ERROR: /home/jovyan/ (unittest.loader._FailedTest)\n",
      "----------------------------------------------------------------------\n",
      "AttributeError: module '__main__' has no attribute '/home/jovyan/'\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.003s\n",
      "\n",
      "FAILED (errors=1)\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "True",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3425: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import unittest\n",
    "\n",
    "# here declare all the variables that are shared among unit test methods.\n",
    "shared_variable = \"foo\"\n",
    "\n",
    "# get the job module to be used given the job name\n",
    "# for example if we want to tests on job_1\n",
    "job_to_test = \"job_1\"\n",
    "\n",
    "\n",
    "# here goes the class and the unit test methods\n",
    "\n",
    "class SparkJobTests(unittest.TestCase):\n",
    "    # methods below represent the test cases you'd want to run on a section of a job as described in the above code example\n",
    "    def method_1(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "\n",
    "# start the testing\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # run all tests main\n",
    "    unittest.main()\n",
    "\n",
    "    # only run a specific test lets say test contained in method_k\n",
    "    suite = unittest.TestSuite()\n",
    "    suite.addTest(Test(\"method_k\"))\n",
    "    runner = unittest.TextTestRunner()\n",
    "    runner.run(suite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 4.8.3\n",
      "  latest version: 4.8.4\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base conda\n",
      "\n",
      "\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /opt/conda\n",
      "\n",
      "  added / updated specs:\n",
      "    - pytest\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    iniconfig-1.0.1            |     pyh9f0ad1d_0           8 KB  conda-forge\n",
      "    more-itertools-8.5.0       |             py_0          37 KB  conda-forge\n",
      "    pluggy-0.13.1              |   py38h32f6830_2          29 KB  conda-forge\n",
      "    py-1.9.0                   |     pyh9f0ad1d_0          74 KB  conda-forge\n",
      "    pytest-6.0.2               |   py38h32f6830_0         419 KB  conda-forge\n",
      "    toml-0.10.1                |     pyh9f0ad1d_0          18 KB  conda-forge\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:         584 KB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  iniconfig          conda-forge/noarch::iniconfig-1.0.1-pyh9f0ad1d_0\n",
      "  more-itertools     conda-forge/noarch::more-itertools-8.5.0-py_0\n",
      "  pluggy             conda-forge/linux-64::pluggy-0.13.1-py38h32f6830_2\n",
      "  py                 conda-forge/noarch::py-1.9.0-pyh9f0ad1d_0\n",
      "  pytest             conda-forge/linux-64::pytest-6.0.2-py38h32f6830_0\n",
      "  toml               conda-forge/noarch::toml-0.10.1-pyh9f0ad1d_0\n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "iniconfig-1.0.1      | 8 KB      | ##################################### | 100% \n",
      "pluggy-0.13.1        | 29 KB     | ##################################### | 100% \n",
      "py-1.9.0             | 74 KB     | ##################################### | 100% \n",
      "pytest-6.0.2         | 419 KB    | ##################################### | 100% \n",
      "toml-0.10.1          | 18 KB     | ##################################### | 100% \n",
      "more-itertools-8.5.0 | 37 KB     | ##################################### | 100% \n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%conda install pytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
